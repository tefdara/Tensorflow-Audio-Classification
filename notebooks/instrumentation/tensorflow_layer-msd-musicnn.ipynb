{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../\")\n",
    "from common.base import MetaData, Processor\n",
    "from common.predictors import TensorflowtMusiCNN, Tensorflow2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we download and save four machine learning models and their associated metadata files. These models are used for audio feature extraction and classification. We define the URLs for the models and metadata files at the beginning of the code, and we use the `os.path.join()` function to define the file paths where the downloaded files will be saved.\n",
    "\n",
    "Before downloading the files, we check if they already exist in the specified file paths using the `os.path.exists()` function. If any of the files don't exist, we use the `wget` command to download the files from the URLs and save them to the specified file paths. We use the `-q` option to suppress output from the `wget` command, so the code doesn't print any messages to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "embedding_model_url = 'https://essentia.upf.edu/models/feature-extractors/musicnn/msd-musicnn-1.pb'\n",
    "embedding_model_json_url = 'https://essentia.upf.edu/models/feature-extractors/musicnn/msd-musicnn-1.json'\n",
    "embedding_model_file = os.path.join('../..', 'models', os.path.basename(embedding_model_url))\n",
    "embedding_model_json = os.path.join('../..', 'models', os.path.basename(embedding_model_json_url))\n",
    "\n",
    "\n",
    "# classification model\n",
    "# this does not work on short sounds\n",
    "model_url = 'https://essentia.upf.edu/models/classification-heads/fs_loop_ds/fs_loop_ds-msd-musicnn-1.pb'\n",
    "model_json_url = 'https://essentia.upf.edu/models/classification-heads/fs_loop_ds/fs_loop_ds-msd-musicnn-1.json'\n",
    "model_file = os.path.join('../..', 'models', os.path.basename(model_url))\n",
    "model_json = os.path.join('../..', 'models', os.path.basename(model_json_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_embedding_model = True if(embedding_model_json_url != '' and embedding_model_url != '') else False\n",
    "if(not os.path.exists(model_json)):\n",
    "    !wget -q $model_json_url -P ../../models\n",
    "if(not os.path.exists(model_file)):\n",
    "    !wget -q $model_url -P ../../models\n",
    "if(has_embedding_model):\n",
    "    if(not os.path.exists(embedding_model_file)):\n",
    "        !wget -q $embedding_model_url -P ../../models\n",
    "    if(not os.path.exists(embedding_model_json)):\n",
    "        !wget -q $embedding_model_json_url -P ../../models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio\n",
    "\n",
    "Audio file can be a single file or a directory of sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file dialog to select an audio file or set open_file_dialog to False and set file_path to the audio file\n",
    "from common.base import file_dialog as fd \n",
    "open_file_dialog = False\n",
    "file_path = '/Users/dtef/Repos/Sync/Audio/Sounds/Sound Libraries/Tef/OK 01/Burning Tornadic Debris Stereo_01/short'\n",
    "if(open_file_dialog):\n",
    "    audio_file = fd()\n",
    "else: audio_file = file_path\n",
    "    \n",
    "processor = Processor(audio_file_path=audio_file)\n",
    "# gets a list of audio files in the selected directory\n",
    "audio_files = processor.audio_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "Next we have to initialise a `MetaData` object with the `model_json` argument, which is assumed to be a file path to a JSON file containing metadata for the TensorFlow model. The `MetaData` object is used to extract the schema and layer information for the model. We do this here since we don't want to initialise a unique instance of the metadata for each audio file we are processing. \n",
    "\n",
    "The `get_schema()` method of the `MetaData` object is used to extract the schema for the model. The schema is a dictionary that contains information about the model's input and output layers, including their names, shapes, and data types. The schema is printed to the console using the `print()` function.\n",
    "\n",
    "The `get_layer()` method of the `MetaData` object is used to extract the input and output layers for the model. The `model_input_layer` and `model_output_layer` variables are assigned the input and output layers, respectively. These variables can be used later in the code to feed data into the model and extract the model's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Embeddings metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected input layer:  model/Placeholder\n",
      "Detected output layer:  model/dense/BiasAdd\n",
      "{'inputs': [{'name': 'model/Placeholder', 'type': 'float', 'shape': [187, 96]}], 'outputs': [{'name': 'model/Sigmoid', 'type': 'float', 'shape': [1, 50], 'op': 'Sigmoid', 'output_purpose': 'predictions'}, {'name': 'model/dense_1/BiasAdd', 'type': 'float', 'shape': [1, 50], 'op': 'fully connected', 'description': 'logits', 'output_purpose': ''}, {'name': 'model/dense/BiasAdd', 'type': 'float', 'shape': [1, 200], 'op': 'fully connected', 'output_purpose': 'embeddings'}]}\n"
     ]
    }
   ],
   "source": [
    "if(has_embedding_model):\n",
    "    embeddings_metadata = MetaData(embedding_model_json)\n",
    "    embeddings_schema = embeddings_metadata.get_schema()\n",
    "\n",
    "    embeddings_input_layer = embeddings_metadata.get_layer('input')\n",
    "    print(\"Detected input layer: \", embeddings_input_layer)\n",
    "    embeddings_output_layer = embeddings_metadata.get_layer('output', 'embeddings')\n",
    "    print(\"Detected output layer: \", embeddings_output_layer)\n",
    "\n",
    "    print(embeddings_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Predictor's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected input layer:  serving_default_model_Placeholder\n",
      "Detected output layer:  PartitionedCall\n",
      "{'inputs': [{'name': 'serving_default_model_Placeholder', 'type': 'float', 'shape': ['batch_size', 200]}], 'outputs': [{'name': 'PartitionedCall', 'type': 'float', 'shape': ['batch_size', 5], 'op': 'Softmax', 'output_purpose': 'predictions'}]}\n"
     ]
    }
   ],
   "source": [
    "classifier_metadata = MetaData(model_json)\n",
    "classifier_schema = classifier_metadata.get_schema()\n",
    "\n",
    "classifier_input_layer = classifier_metadata.get_layer('input')\n",
    "print(\"Detected input layer: \", classifier_input_layer)\n",
    "classifier_output_layer = classifier_metadata.get_layer('output', 'predictions')\n",
    "print(\"Detected output layer: \", classifier_output_layer)\n",
    "\n",
    "print(classifier_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is trained on the VGGish embeddings, we need to use the same schema. We could use the `TensorflowPredictVGGish` algorithm because it generates the required mel-spectrogram. This approach won't need any embeddings. But for better learning transfer and noise reduction, the embeddings can be inputed to `TensorflowPredict2D`. \n",
    "\n",
    "The extractor and classifier used bellow exist in the classifiers.py file. This to reduce the size of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burning_short_9.wav  Label:  bass  Probability:  5.7%\n",
      "Burning_short_9.wav  Label:  chords  Probability:  0.7%\n",
      "Burning_short_9.wav  Label:  fx  Probability:  91.4%\n",
      "Burning_short_9.wav  Label:  melody  Probability:  0.7%\n",
      "Burning_short_9.wav  Label:  percussion  Probability:  1.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `../../models/msd-musicnn-1.pb`\n",
      "2023-08-28 16:12:39.631979: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-08-28 16:12:39.633167: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `../../models/fs_loop_ds-msd-musicnn-1.pb`\n",
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `../../models/msd-musicnn-1.pb`\n",
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `../../models/fs_loop_ds-msd-musicnn-1.pb`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error cannot convert argument LIST_EMPTY to MATRIX_REAL",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/essentia/standard.py:97\u001b[0m, in \u001b[0;36m_create_essentia_class.<locals>.Algo.compute\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     convertedData \u001b[39m=\u001b[39m _c\u001b[39m.\u001b[39;49mconvertData(arg, goalType)\n\u001b[1;32m     98\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/essentia/common.py:349\u001b[0m, in \u001b[0;36mconvertData\u001b[0;34m(data, goalType)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mreturn\u001b[39;00m [[col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m row] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m data]\n\u001b[0;32m--> 349\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot convert data from type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) to type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    350\u001b[0m                 (\u001b[39mstr\u001b[39m(origType), \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(data)), \u001b[39mstr\u001b[39m(goalType)))\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert data from type LIST_EMPTY (<class 'list'>) to type MATRIX_REAL",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m extractor \u001b[39m=\u001b[39m TensorflowtMusiCNN(embedding_model_file, embeddings_input_layer, embeddings_output_layer)\n\u001b[1;32m      2\u001b[0m classifier \u001b[39m=\u001b[39m Tensorflow2D(model_file, classifier_input_layer, classifier_output_layer)\n\u001b[0;32m----> 3\u001b[0m stats \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39;49mclassify_audio(classifier, classifier_metadata, extractor)\n",
      "File \u001b[0;32m~/Repos/Sync/Audio/Tools/Tensorflow-Audio-Classification/notebooks/instrumentation/../../common/base.py:144\u001b[0m, in \u001b[0;36mProcessor.classify_audio\u001b[0;34m(self, model, classifier_metadata, embedding_model)\u001b[0m\n\u001b[1;32m    141\u001b[0m     embeddings \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mcompute(audio_file\u001b[39m=\u001b[39maudio)\n\u001b[1;32m    143\u001b[0m classifier \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(model\u001b[39m.\u001b[39mgraph_path, model\u001b[39m.\u001b[39minput_layer, model\u001b[39m.\u001b[39moutput_layer)\n\u001b[0;32m--> 144\u001b[0m classifier\u001b[39m.\u001b[39;49mcompute(audio_file\u001b[39m=\u001b[39;49maudio, embeddings\u001b[39m=\u001b[39;49membeddings)\n\u001b[1;32m    145\u001b[0m stat \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mevaluate(classifier_metadata\u001b[39m.\u001b[39mdata)\n\u001b[1;32m    146\u001b[0m file_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(audio)\n",
      "File \u001b[0;32m~/Repos/Sync/Audio/Tools/Tensorflow-Audio-Classification/notebooks/instrumentation/../../common/predictors.py:18\u001b[0m, in \u001b[0;36mTensorflow2D.compute\u001b[0;34m(self, audio_file, embeddings)\u001b[0m\n\u001b[1;32m     12\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_audio_or_embeddings(audio_file, embeddings)\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39mTensorflowPredict2D(\n\u001b[1;32m     14\u001b[0m     graphFilename\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_path),\n\u001b[1;32m     15\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer,\n\u001b[1;32m     16\u001b[0m     output\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictions \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictions\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/essentia/standard.py:123\u001b[0m, in \u001b[0;36m_create_essentia_class.<locals>.Algo.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/essentia/standard.py:99\u001b[0m, in \u001b[0;36m_create_essentia_class.<locals>.Algo.compute\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     97\u001b[0m         convertedData \u001b[39m=\u001b[39m _c\u001b[39m.\u001b[39mconvertData(arg, goalType)\n\u001b[1;32m     98\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError cannot convert argument \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    100\u001b[0m               \u001b[39m%\u001b[39m(\u001b[39mstr\u001b[39m(_c\u001b[39m.\u001b[39mdetermineEdt(arg)), \u001b[39mstr\u001b[39m(goalType)))\n\u001b[1;32m    102\u001b[0m     convertedArgs\u001b[39m.\u001b[39mappend(convertedData)\n\u001b[1;32m    104\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__compute__(\u001b[39m*\u001b[39mconvertedArgs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Error cannot convert argument LIST_EMPTY to MATRIX_REAL"
     ]
    }
   ],
   "source": [
    "extractor = TensorflowtMusiCNN(embedding_model_file, embeddings_input_layer, embeddings_output_layer)\n",
    "classifier = Tensorflow2D(model_file, classifier_input_layer, classifier_output_layer)\n",
    "stats = processor.classify_audio(classifier, classifier_metadata, extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, to export the results, we iterate over the list of audio files, extract their classification statistics, and save the statistics to a JSON files. If there is an analysis file dedicated to that audio file with the naming scheme defined bellow, we just add or update the values to the dictionary. At the end, we prompt the os to open the directory containing the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_folder = 'Moods'\n",
    "stats_parent = 'Agressive'\n",
    "processor.export_data(stats, stats_folder, stats_parent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
