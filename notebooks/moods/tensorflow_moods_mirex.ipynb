{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../\")\n",
    "from common.base import MetaData, Processor\n",
    "from common.predictors import OpenL3, Tensorflow2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we download and save four machine learning models and their associated metadata files. These models are used for audio feature extraction and classification. We define the URLs for the models and metadata files at the beginning of the code, and we use the `os.path.join()` function to define the file paths where the downloaded files will be saved.\n",
    "\n",
    "Before downloading the files, we check if they already exist in the specified file paths using the `os.path.exists()` function. If any of the files don't exist, we use the `wget` command to download the files from the URLs and save them to the specified file paths. We use the `-q` option to suppress output from the `wget` command, so the code doesn't print any messages to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "embedding_model_url = 'https://essentia.upf.edu/models/feature-extractors/openl3/openl3-env-mel256-emb6144-3.pb'\n",
    "embedding_model_json_url = 'https://essentia.upf.edu/models/feature-extractors/openl3/openl3-env-mel256-emb6144-3.json'\n",
    "embedding_model_file = os.path.join('../..', 'models', os.path.basename(embedding_model_url))\n",
    "embedding_model_json = os.path.join('../..', 'models', os.path.basename(embedding_model_json_url))\n",
    "\n",
    "\n",
    "# classification model\n",
    "model_url = 'https://essentia.upf.edu/models/classifiers/moods_mirex/moods_mirex-vggish-audioset-1.pb'\n",
    "model_json_url = 'https://essentia.upf.edu/models/classifiers/moods_mirex/moods_mirex-vggish-audioset-1.json'\n",
    "model_file = os.path.join('../..', 'models', os.path.basename(model_url))\n",
    "model_json = os.path.join('../..', 'models', os.path.basename(model_json_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_embedding_model = True if(embedding_model_json_url != '' and embedding_model_url != '') else False\n",
    "if(not os.path.exists(model_json)):\n",
    "    !wget -q $model_json_url -P ../../models\n",
    "if(not os.path.exists(model_file)):\n",
    "    !wget -q $model_url -P ../../models\n",
    "if(has_embedding_model):\n",
    "    if(not os.path.exists(embedding_model_file)):\n",
    "        !wget -q $embedding_model_url -P ../../models\n",
    "    if(not os.path.exists(embedding_model_json)):\n",
    "        !wget -q $embedding_model_json_url -P ../../models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio\n",
    "\n",
    "Audio file can be a single file or a directory of sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file dialog to select an audio file or set open_file_dialog to False and set file_path to the audio file\n",
    "from common.base import file_dialog as fd \n",
    "open_file_dialog = False\n",
    "file_path = '~/Desktop/test/'\n",
    "if(open_file_dialog):\n",
    "    audio_file = fd()\n",
    "else: audio_file = file_path\n",
    "    \n",
    "processor = Processor(audio_file_path=audio_file)\n",
    "# gets a list of audio files in the selected directory\n",
    "audio_files = processor.audio_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "Next we have to initialise a `MetaData` object with the `model_json` argument, which is assumed to be a file path to a JSON file containing metadata for the TensorFlow model. The `MetaData` object is used to extract the schema and layer information for the model. We do this here since we don't want to initialise a unique instance of the metadata for each audio file we are processing. \n",
    "\n",
    "The `get_schema()` method of the `MetaData` object is used to extract the schema for the model. The schema is a dictionary that contains information about the model's input and output layers, including their names, shapes, and data types. The schema is printed to the console using the `print()` function.\n",
    "\n",
    "The `get_layer()` method of the `MetaData` object is used to extract the input and output layers for the model. The `model_input_layer` and `model_output_layer` variables are assigned the input and output layers, respectively. These variables can be used later in the code to feed data into the model and extract the model's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Embeddings metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(has_embedding_model):\n",
    "    embeddings_metadata = MetaData(embedding_model_json)\n",
    "    embeddings_schema = embeddings_metadata.get_schema()\n",
    "\n",
    "    embeddings_input_layer = embeddings_metadata.get_layer('input')\n",
    "    print(\"Detected input layer: \", embeddings_input_layer)\n",
    "    embeddings_output_layer = embeddings_metadata.get_layer('output', 'predictions')\n",
    "    print(\"Detected output layer: \", embeddings_output_layer)\n",
    "\n",
    "    print(embeddings_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Predictor's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_metadata = MetaData(model_json)\n",
    "classifier_schema = classifier_metadata.get_schema()\n",
    "\n",
    "classifier_input_layer = classifier_metadata.get_layer('input')\n",
    "print(\"Detected input layer: \", classifier_input_layer)\n",
    "classifier_output_layer = classifier_metadata.get_layer('output', 'predictions')\n",
    "print(\"Detected output layer: \", classifier_output_layer)\n",
    "\n",
    "print(classifier_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is trained on the VGGish embeddings, we need to use the same schema. We could use the `TensorflowPredictVGGish` algorithm because it generates the required mel-spectrogram. This approach won't need any embeddings. But for better learning transfer and noise reduction, the embeddings can be inputed to `TensorflowPredict2D`. \n",
    "\n",
    "The extractor and classifier used bellow exist in the classifiers.py file. This to reduce the size of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = OpenL3(embedding_model_file, embeddings_input_layer, embeddings_output_layer)\n",
    "classifier = Tensorflow2D(model_file, classifier_input_layer, classifier_output_layer)\n",
    "stats = processor.classify_audio(classifier, classifier_metadata, extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, to export the results, we iterate over the list of audio files, extract their classification statistics, and save the statistics to a JSON files. If there is an analysis file dedicated to that audio file with the naming scheme defined bellow, we just add or update the values to the dictionary. At the end, we prompt the os to open the directory containing the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_folder = 'Moods'\n",
    "stats_parent = 'MIReX'\n",
    "processor.export_data(stats, stats_folder, stats_parent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
